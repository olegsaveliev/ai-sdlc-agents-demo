name: QA Agent - Auto Test Generation

on:
  # Run on push to any branch (including feature branches)
  push:
    branches:
      - '**'        # All branches
      - '!main'     # Exclude main branch (optional - remove if you want it to run on main too)
  
  # Run on pull requests
  pull_request:
    types: [opened, synchronize, reopened]
  
  # Allow manual trigger
  workflow_dispatch:

jobs:
  qa-agent:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: write
      issues: write
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          # For PR: checkout the PR branch
          # For push: checkout the pushed branch
          ref: ${{ github.event.pull_request.head.ref || github.ref }}
          fetch-depth: 0  # Get full history for better context
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install anthropic requests pytest dnspython
          
          # Install app dependencies if requirements.txt exists
          if [ -f "requirements.txt" ]; then
            echo "üì¶ Installing application dependencies..."
            pip install -r requirements.txt
          else
            echo "‚ÑπÔ∏è No requirements.txt found - skipping app dependencies"
          fi

      - name: Copy Slack notifications module
        run: |
          if [ -f "slack_notifications.py" ]; then
            cp slack_notifications.py agents/ || mkdir -p agents && cp slack_notifications.py agents/
            echo "‚úÖ Slack notifications module copied"
          else
            echo "‚ö†Ô∏è slack_notifications.py not found - skipping"
          fi
      
      - name: Determine PR number
        id: pr_info
        run: |
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            echo "pr_number=${{ github.event.pull_request.number }}" >> $GITHUB_OUTPUT
            echo "is_pr=true" >> $GITHUB_OUTPUT
          else
            # For push events, try to find associated PR
            PR_NUMBER=$(gh pr list --head "${{ github.ref_name }}" --json number --jq '.[0].number' 2>/dev/null || echo "")
            if [ -n "$PR_NUMBER" ]; then
              echo "pr_number=$PR_NUMBER" >> $GITHUB_OUTPUT
              echo "is_pr=true" >> $GITHUB_OUTPUT
              echo "üìå Found associated PR #$PR_NUMBER"
            else
              echo "is_pr=false" >> $GITHUB_OUTPUT
              echo "‚ÑπÔ∏è No PR associated with this push"
            fi
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Run QA Agent
        if: steps.pr_info.outputs.is_pr == 'true' || github.event_name == 'push'
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }} 
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          PR_NUMBER: ${{ steps.pr_info.outputs.pr_number }}
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
          NOTION_METRICS_DB_ID: ${{ secrets.NOTION_METRICS_DB_ID }}
          BRANCH_NAME: ${{ github.ref_name }}
          EVENT_TYPE: ${{ github.event_name }}
        run: |
          echo "ü§ñ Running QA Agent..."
          echo "Branch: $BRANCH_NAME"
          echo "Event: $EVENT_TYPE"
          
          if [ -f "agents/qa_agent.py" ]; then
            python agents/qa_agent.py
          else
            echo "‚ö†Ô∏è agents/qa_agent.py not found - skipping QA agent execution"
          fi
      
      - name: Run Generated Tests
        continue-on-error: true
        run: |
          if [ -f "tests/test_generated.py" ]; then
            echo "üß™ Running generated tests..."
            export PYTHONPATH="${PYTHONPATH}:$(pwd)"
            pytest tests/test_generated.py -v --tb=short > test_results.txt 2>&1 || true
            echo "‚úÖ Tests executed"
          else
            echo "‚ÑπÔ∏è No tests generated" > test_results.txt
          fi
      
      - name: Post Results to GitHub and Notion
        if: always()
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          PR_NUMBER: ${{ steps.pr_info.outputs.pr_number }}
          IS_PR: ${{ steps.pr_info.outputs.is_pr }}
          NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
          NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
          BRANCH_NAME: ${{ github.ref_name }}
          EVENT_TYPE: ${{ github.event_name }}
          COMMIT_SHA: ${{ github.sha }}
        run: |
          python3 << 'PYTHON_SCRIPT'
          import os
          import requests
          import re
          import sys
          
          # Read test results
          try:
              with open('test_results.txt', 'r') as f:
                  test_results = f.read()
          except FileNotFoundError:
              test_results = "No test results available"
          
          # Parse test metrics
          def parse_test_metrics(results):
              metrics = {
                  'total': 0,
                  'passed': 0,
                  'failed': 0,
                  'errors': 0,
                  'skipped': 0,
                  'completion_pct': 0,
                  'test_names': []
              }
              
              passed_match = re.search(r'(\d+) passed', results)
              failed_match = re.search(r'(\d+) failed', results)
              error_match = re.search(r'(\d+) error', results)
              skipped_match = re.search(r'(\d+) skipped', results)
              
              if passed_match:
                  metrics['passed'] = int(passed_match.group(1))
              if failed_match:
                  metrics['failed'] = int(failed_match.group(1))
              if error_match:
                  metrics['errors'] = int(error_match.group(1))
              if skipped_match:
                  metrics['skipped'] = int(skipped_match.group(1))
              
              metrics['total'] = metrics['passed'] + metrics['failed'] + metrics['errors'] + metrics['skipped']
              
              if metrics['total'] > 0:
                  metrics['completion_pct'] = round((metrics['passed'] / metrics['total']) * 100, 1)
              
              test_pattern = r'(test_\w+)\s+(PASSED|FAILED|ERROR|SKIPPED)'
              for match in re.finditer(test_pattern, results):
                  test_name = match.group(1)
                  status = match.group(2)
                  metrics['test_names'].append({
                      'name': test_name,
                      'status': status
                  })
              
              return metrics
          
          metrics = parse_test_metrics(test_results)
          
          # Get environment variables
          repo = os.environ['GITHUB_REPOSITORY']
          token = os.environ['GITHUB_TOKEN']
          pr_number = os.environ.get('PR_NUMBER', '')
          is_pr = os.environ.get('IS_PR', 'false')
          branch_name = os.environ.get('BRANCH_NAME', 'unknown')
          event_type = os.environ.get('EVENT_TYPE', 'unknown')
          commit_sha = os.environ.get('COMMIT_SHA', '')[:7]
          
          headers = {
              "Authorization": f"Bearer {token}",
              "Accept": "application/vnd.github+json"
          }
          
          # Determine context
          if is_pr == 'true' and pr_number:
              context_title = f"PR #{pr_number}"
              try:
                  pr_response = requests.get(
                      f"https://api.github.com/repos/{repo}/pulls/{pr_number}",
                      headers=headers,
                      timeout=10
                  )
                  pr_title = pr_response.json().get('title', 'Unknown PR')
              except Exception as e:
                  print(f"‚ö†Ô∏è Could not fetch PR details: {e}")
                  pr_title = "Unknown PR"
          else:
              context_title = f"Branch: {branch_name}"
              pr_title = f"Commit {commit_sha} on {branch_name}"
          
          # Post to GitHub (only if PR exists)
          if is_pr == 'true' and pr_number:
              gh_comment = f"""## üß™ QA Agent Report
          
**Automated Test Generation Complete**
**Branch:** `{branch_name}` | **Event:** `{event_type}` | **Commit:** `{commit_sha}`

### üìä Test Summary
- **Total Tests:** {metrics['total']}
- **Passed:** ‚úÖ {metrics['passed']}
- **Failed:** ‚ùå {metrics['failed']}
- **Errors:** üî¥ {metrics['errors']}
- **Completion:** {metrics['completion_pct']}%

<details>
<summary>üìã Detailed Results</summary>

```
{test_results[:2000]}
```
</details>

---
*Generated by QA Agent on {event_type}*"""
              
              try:
                  gh_response = requests.post(
                      f"https://api.github.com/repos/{repo}/issues/{pr_number}/comments",
                      headers=headers,
                      json={"body": gh_comment},
                      timeout=30
                  )
                  
                  if gh_response.status_code == 201:
                      print(f"‚úÖ Posted to GitHub PR #{pr_number} successfully!")
                  else:
                      print(f"‚ö†Ô∏è GitHub post failed: {gh_response.status_code}")
                      print(f"Response: {gh_response.text[:200]}")
              except Exception as e:
                  print(f"‚ùå Error posting to GitHub: {e}")
          else:
              print(f"‚ÑπÔ∏è Skipping GitHub comment (no PR associated)")
              print(f"   Context: {context_title}")
              print(f"   Tests: {metrics['passed']}/{metrics['total']} passed")
          
          # Post to Notion with detailed metrics
          notion_token = os.environ.get('NOTION_TOKEN')
          notion_db = os.environ.get('NOTION_DATABASE_ID')
          
          if notion_token and notion_db:
              print("üìù Posting to Notion...")
              
              notion_headers = {
                  "Authorization": f"Bearer {notion_token}",
                  "Content-Type": "application/json",
                  "Notion-Version": "2022-06-28"
              }
              
              test_breakdown = []
              
              test_breakdown.append({
                  "object": "block",
                  "type": "heading_2",
                  "heading_2": {
                      "rich_text": [{"type": "text", "text": {"content": "üìä Test Execution Summary"}}]
                  }
              })
              
              metrics_text = f"Total: {metrics['total']} | Passed: {metrics['passed']} | Failed: {metrics['failed']} | Completion: {metrics['completion_pct']}%"
              test_breakdown.append({
                  "object": "block",
                  "type": "callout",
                  "callout": {
                      "rich_text": [{"type": "text", "text": {"content": metrics_text}}],
                      "icon": {"emoji": "üìà"}
                  }
              })
              
              # Add branch/commit info
              test_breakdown.append({
                  "object": "block",
                  "type": "paragraph",
                  "paragraph": {
                      "rich_text": [{"type": "text", "text": {"content": f"Branch: {branch_name} | Event: {event_type} | Commit: {commit_sha}"}}]
                  }
              })
              
              if metrics['test_names']:
                  test_breakdown.append({
                      "object": "block",
                      "type": "heading_3",
                      "heading_3": {
                          "rich_text": [{"type": "text", "text": {"content": "Test Cases Executed"}}]
                      }
                  })
                  
                  for test in metrics['test_names'][:20]:
                      emoji = "‚úÖ" if test['status'] == "PASSED" else "‚ùå" if test['status'] == "FAILED" else "üî¥"
                      test_text = f"{emoji} {test['name']} - {test['status']}"
                      
                      test_breakdown.append({
                          "object": "block",
                          "type": "bulleted_list_item",
                          "bulleted_list_item": {
                              "rich_text": [{"type": "text", "text": {"content": test_text}}]
                          }
                      })
              
              test_breakdown.append({
                  "object": "block",
                  "type": "heading_3",
                  "heading_3": {
                      "rich_text": [{"type": "text", "text": {"content": "Raw Test Output"}}]
                  }
              })
              
              truncated_results = test_results[:1500] if len(test_results) > 1500 else test_results
              test_breakdown.append({
                  "object": "block",
                  "type": "code",
                  "code": {
                      "rich_text": [{"type": "text", "text": {"content": truncated_results}}],
                      "language": "plain text"
                  }
              })
              
              notion_data = {
                  "parent": {"database_id": notion_db},
                  "properties": {
                      "Name": {
                          "title": [{"text": {"content": f"QA: {pr_title[:70]} ({metrics['completion_pct']}%)"}}]
                      },
                      "Agent Type": {
                          "select": {"name": "QA"}
                      },
                      "Status": {
                          "select": {"name": "Complete"}
                      }
                  },
                  "children": [
                      {
                          "object": "block",
                          "type": "heading_2",
                          "heading_2": {
                              "rich_text": [{"type": "text", "text": {"content": "üß™ QA Test Report"}}]
                          }
                      },
                      {
                          "object": "block",
                          "type": "paragraph",
                          "paragraph": {
                              "rich_text": [{"type": "text", "text": {"content": pr_title}}]
                          }
                      },
                      {
                          "object": "block",
                          "type": "divider",
                          "divider": {}
                      }
                  ] + test_breakdown
              }
              
              # Add PR number if available
              if pr_number:
                  try:
                      notion_data["properties"]["Issue/PR Number"] = {
                          "number": int(pr_number)
                      }
                  except ValueError:
                      pass
              
              try:
                  notion_response = requests.post(
                      "https://api.notion.com/v1/pages",
                      headers=notion_headers,
                      json=notion_data,
                      timeout=30
                  )
                  
                  if notion_response.status_code == 200:
                      print("‚úÖ Posted to Notion successfully!")
                      print(f"   - Total Tests: {metrics['total']}")
                      print(f"   - Passed: {metrics['passed']}")
                      print(f"   - Completion: {metrics['completion_pct']}%")
                  else:
                      print(f"‚ö†Ô∏è Notion post failed: {notion_response.status_code}")
                      print(f"Response: {notion_response.text[:200]}")
              except Exception as e:
                  print(f"‚ùå Error posting to Notion: {e}")
          else:
              print("‚ÑπÔ∏è Notion credentials not configured")
          
          # Print summary
          print("\n" + "="*50)
          print(f"üìä QA Agent Summary")
          print(f"   Context: {context_title}")
          print(f"   Branch: {branch_name}")
          print(f"   Tests Run: {metrics['total']}")
          print(f"   Passed: {metrics['passed']}")
          print(f"   Failed: {metrics['failed']}")
          print(f"   Success Rate: {metrics['completion_pct']}%")
          print("="*50)
          
          PYTHON_SCRIPT
